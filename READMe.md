# ðŸ¤– AI-Powered Kubernetes Log Analyzer

An interactive Streamlit dashboard that uses machine learning to classify Kubernetes pod logs into `error`, `warning`, and `info` levels â€” helping you quickly identify and investigate issues in your workloads.

![dashboard screenshot](docs/screenshot.png)

## ðŸš€ Features

- Automatically loads `.log` or `.txt` files from the `sample_logs/` directory
- Uses machine learning (Logistic Regression) to classify logs
- Color-coded table with âœ… info, âš ï¸ warning, âŒ error labels
- Bar chart showing severity distribution
- Upload your own logs or use collected pod logs

## ðŸ› ï¸ Tech Stack

- Python 3.x
- Streamlit
- scikit-learn
- Kubernetes
- Optional: Minikube or AWS EKS

## ðŸ“ Project Structure

kube-log-ai-analyzer/
â”œâ”€â”€ dashboard.py
â”œâ”€â”€ collect_logs.py
â”œâ”€â”€ train_model.py
â”œâ”€â”€ train.csv
â”œâ”€â”€ log_classifier.pkl
â”œâ”€â”€ sample_logs/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
## âš™ï¸ How to Run

Follow these steps to set up and run the AI-powered Kubernetes log analyzer on your machine or EC2 instance.

---

### ðŸ§± Step 1: Clone the Repository

```bash
git clone https://github.com/bhanup6/kube-log-ai-analyzer.git
cd kube-log-ai-analyzer
```

---

### ðŸ Step 2: Set Up Python Environment

```bash
python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

---

### ðŸ§  Step 3: Train the AI Model

Make sure `train.csv` contains your labeled log data in this format:

```csv
log,severity
"nginx started successfully",info
"retrying API call",warning
"disk quota exceeded",error
```

Then train the model:

```bash
python train_model.py
```

> This generates a trained model file: `log_classifier.pkl`

---

### ðŸ“¥ Step 4: Collect Logs from Kubernetes (Optional)

If you have a Kubernetes cluster (like Minikube or EKS), run:

```bash
python collect_logs.py
```

> This saves logs from your running pods into the `sample_logs/` folder.

**OR** you can manually place `.log` or `.txt` files into the `sample_logs/` directory.

---

### ðŸ–¥ï¸ Step 5: Run the Dashboard

```bash
streamlit run dashboard.py
```

Then open in your browser:

```
http://localhost:8501
```

> If you're running this on an EC2 server, open:
> ```
> http://<your-ec2-public-ip>:8501
> ```

---

### âœ… Quick Start (All in One)

```bash
git clone https://github.com/bhanup6/kube-log-ai-analyzer.git
cd kube-log-ai-analyzer
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python train_model.py
python collect_logs.py   # optional
streamlit run dashboard.py
```

---

### ðŸ§ª Example Logs

Your `.log` files should contain one log message per line, e.g.:

```log
nginx started successfully
retrying API call
connection refused on port 443
disk quota exceeded
cleanup completed successfully
```

---

Thatâ€™s it! ðŸŽ‰ You're now ready to analyze logs using AI!


